# Más sobre problemas de clasificación

En esta parte presentamos técnicas adicionales para evaluar el 
desempeño de un modelo. En la parte anterior vimos que

- La **devianza** es una buena medida para ajustar y evaluar el desempeño de un modelo y 
comparar modelos, y utilizar las probabilidades de clase. Sin embargo, es una medida de dificil de interpretar en cuanto 
a los errores que podemos esperar del modelo.

- Por otro lado, la **tasa de clasificación incorrecta** puede
usarse para evaluar el desempeño de un clasificador 
(incluyendo uno derivado de probabilidades de clase), puede interpretarse
con facilidad,
pero se queda corta en muchas aplicaciones. Una deficiencia grande
de esta medida es que, contrario al problema de regresión, hay errores
de clasificación que son cualitativamente diferentes.

#### Ejemplo {-}
- Por ejemplo, diagnosticar a alguien con una enfermedad cuando no la tiene
tiene consecuencias distintas a diagnosticar como libre de enfermedad a alguien
que la tiene. Estas consecuencias dependen de cómo son son los tratamientos consecuentes, de y qué tan peligrosa es la enfermedad.

- Cuando usamos un buscador como Google, es cualitativamente diferente que el
buscador omita resultados relevantes a que nos presente resultados irrelevantes.

- ¿Otros ejemplos?

En general, los costos de los distintos errores son distintos, y en muchos
problemas quiséramos entenderlos y controlarlos individualmente. Aunque en teoría
podríamos asignar costos a los errores y definir una función de pérdida apropiada,
en la práctica esto muchas veces no es tan fácil o deseable. Podemos, sin embargo,
reportar el tipo de errores que ocurren

```{block2, type='comentario'}
**Matriz de confusión**.
Sea $\hat{G}$ un clasificador binario. La matriz de confusión $C$ de $\hat{G}$ está 
dada por

$C_{i,j} = $\text{Número de casos de la clase verdadera j que son clasificados como clase i
 por el clasificador}$
```

#### Ejemplo {-} 

En un ejemplo de tres clases, podríamos obtener la matriz de confusión:

```{r, echo=FALSE}
tabla_1 <- data.frame(A=c(50,20,20), B=c(2,105,10), C=c(0,10,30))
rownames(tabla_1) <- c('A.pred', 'B.pred', 'C.pred')
tabla_1 <- as.table(as.matrix(tabla_1))
knitr::kable(tabla_1)
```

Esto quiere decir que de 90 casos de clase $A$, sólo clasificamos
a 50 en la clase correcta, de 117 casos de clase $B$, acertamos en 105, etcétera.
Podemos ver esta tabla de distintas formas, por ejemplo, usando porcentajes
por columna, nos dice cómo se distribuyen los casos de cada clase:

```{r}
knitr::kable(round(prop.table(tabla_1, 2),2))
```

Mientras que una tabla de porcentajes por renglón nos muestra
qué pasa cada vez que hacemos una predicción dada:

```{r}
knitr::kable(round(prop.table(tabla_1, 1),2))
```

Ahora pensemos cómo podría sernos de utilidad esta tabla. Discute

- El clasificador fuera uno de severidad de emergencias en un hospital,
donde A=requiere atención inmediata B=urgente C=puede posponerse.
- El clasificador fuera el tipo de producto que compran  clientes de un negocio
A=producto premium, B=producto estándar, C=no compra`. Imagínate
que el producto premium es caro pero requiere de atención personal de
un agente de ventas para ser vendido.

La tasa de incorrectas es la misma en los dos ejemplos, pero la adecuación
del clasificador es muy diferente.

## Análisis de error para clasificadores binarios

Cuando la variable a predecir es binaria (dos clases), podemos
etiquetar una clase como *positivo* y otra como *negativo*. En el fondo
no importa cómo catalogemos cada clase, pero para problemas particulares
una asignación puede ser más natural. Por ejemplo, en diagnóstico de 
enfermedades, positivo=tiene la enfermedad, en análisis de crédito,
positivo=cae en impago, en sistemas de recomendacion, positivo = le gusta
el producto X, en recuperación de textos, positivo=el documento es relevante a la
búsqueda, etc.


```{block2, type='comentario'}
Hay dos tipos de errores en un clasificador binario (positivo - negativo):

- Falsos positivos (fp): clasificar como positivo a un caso negativo.
- Falsos negativos (fn): clasificar como negativo a un caso positivo.

A los casos clasificados correctamente les llamamos positivos verdaderos (pv)
y negativos verdaderos (nv).
```

La matriz de confusion es entonces

\begin{tabular*}{r|cc}
 & positivo & negativo \\
 \hline
positivo.pred & pv & fp \\
negativo.pred & fn & nv \\
\end{tabular*}

Nótese que un clasificador bueno, en general, es uno
que tiene la mayor parte de los casos en la diagonal de la matriz
de confusión.

Podemos estudiar a nuestro clasificador en términos de las proporciones de casos que caen en cada celda, que dependen del desempeño del clasificador en cuanto a casos positivos y negativos. La nomenclatura es
confusa, pues en distintas áreas se usan distintos nombres para estas proporciones:

- Tasa de falsos positivos
$$\frac{fp}{fp+nv}=\frac{fp}{negativos}$$

- Tasa de falsos negativos
$$\frac{fn}{pv+fn}=\frac{fn}{positivos}$$

- Especificidad
$$\frac{nv}{fp+nv}=\frac{nv}{negativos}$$

- Sensibilidad o Recall
$$\frac{pv}{pv+fn}=\frac{pv}{positivos}$$ 

Y también otras que tienen como base las predicciones:

- Valor predictivo positivo o Precisión
$$\frac{vp}{vp+fp}=\frac{vp}{pred.positivo}$$

- Valor predictivo negativo
$$\frac{vn}{fn+vn}=\frac{vp}{pred.negativo}$$

Y veremos otras que toman en cuenta ambos tipos de errores

- Tasa de clasificación incorrecta
- Medida F
- AUC (area bajo la curva ROC)
- Kappa 


Dependiendo de el tema y el objetivo hay medidas más naturales que otras:

- En pruebas clínicas, se usa típicamente sensibilidad y especificidad (proporción de positivos que detectamos y proporción de negativos que descartamos).
- En búsqueda y recuperación de documentos (positivo=el documento es relevante, negativo=el documento no es relevante), se usa precisión y recall (precisión=de los documentos que entregamos (predicción positiva), cuáles son realmente positivos/relevantes, y recall=de todos los documentos relevantes, cuáles devolvemos). Aquí la tasa de falsos positivos (de todos los negativos, cuáles se predicen positivos), por ejemplo, no es de ayuda pues generalmente son bajas y no discriminan el desempeño de los clasificadores. La razón es que típicamente hay una gran cantidad de negativos, y se devuelven relativamente pocos documentos, de forma que la tasa de falsos positivos generalmente es muy pequeña.

#### Ejercicio {-}
¿Qué relaciones hay entre las cantidades mostradas arriba? 
Por ejemplo: Escribe la tasa de clasificación incorrecta en términos
de especificidad y sensibilidad.
También intenta escribir valor predictivo positivo y valor predictivo negativo en términos de sensibilidad y especificidad.


```{block2, type='comentario'}
Cada clasificador tiene un balance distinto especificidad-sensibliidad. Muchas veces no escogemos clasificadores por la tasa
de incorrectos solamente, sino que intentamos buscar un balance ade- cuado entre el comportamiento de clasificación para positivos y para negativos.
```

#### Ejercicio {-}
Calcular la matriz de confusión (sobre la muestra de prueba) para el
clasificador logístico de diabetes en términos de glucosa. Calcula 
adicionalmente con la muestra de prueba sus valores de especificidad y sensibilidad, y precisión y recall.

```{r, warnings=FALSE, messages=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
diabetes_ent <- as_data_frame(MASS::Pima.tr)
diabetes_pr <- as_data_frame(MASS::Pima.te)
mod_1 <- glm(type ~ glu, data = diabetes_ent, family = 'binomial')
preds_prueba <- predict(mod_1, newdata = diabetes_pr, type ='response')
```


### Punto de corte para un clasificador binario

¿Qué sucede cuando el perfil de sensibilidad y especificidad de un 
clasificador binario no es apropiado para nuestros fines?
Recordemos que una vez que hemos estimado con $\hat{p}_1(x)$, nuestra regla de clasificación es:

1. Predecir positivo si $\hat{p}_1(x) > 0.5$, 
2. Predecir negativo si $\hat{p}_1(x) < 0.5.$

Esto sugiere una regla alternativa:


Para $0 < d < 1$, podemos utilizar nuestras estimaciones $\hat{p}_1(x)$ para construir un clasificador alternativo poniendo:

1. Predecir positivo si $\hat{p}_1(x) > d$, 
2. Predecir negativo si $\hat{p}_1(x) < d$.


Distintos valores de $d$ dan distintos perfiles de sensibilidad-especificidad para una misma estimación de las probabilidades condicionales de clase:
Para minimizar la tasa de incorrectos conviene poner d = 0.5. Sin embargo, es común que este no es el único fin de un clasificador bueno (pensar en ejemplo de fraude).

- Cuando incrementamos d, quiere decir que exigimos estar más seguros de que un caso es positivo para clasificarlo como positivo. Eso quiere decir que la especifidad va a ser más grande (entre
los negativos verdaderos va a haber menos falsos positivos). Sin embargo, la sensibilidad va a ser más chica pues captamos menos de los verdaderos positivos.

#### Ejemplo {-}
Por ejemplo, si en el caso de diabetes incrementamos el punto de corte a 0.7:
```{r}
table(preds_prueba > 0.7, diabetes_pr$type)
tab <- prop.table(table(preds_prueba > 0.7, diabetes_pr$type),2)
tab
```

La especificidad ahora `r round(tab[1,1],2)` , muy alta (descartamos muy bien casos negativos), pero la sensibilidad se deteriora a `r round(tab[2,2],2)`


- Cuando hacemos más chico d, entonces exigimos estar más segu- ros de que un caso es negativo para clasificarlo como negativo. Esto aumenta la sensibilidad, pero la especificidad baja.
Por ejemplo, si en el caso de diabetes ponemos el punto de corte en 0.3:
```{r}
table(preds_prueba > 0.3, diabetes_pr$type)
tab <- prop.table(table(preds_prueba > 0.3, diabetes_pr$type),2)
tab
```

### Espacio ROC de clasificadores

Podemos visualizar el desempeño de cada uno de estos clasificadores
mapeándolos a las coordenadas de tasa de falsos positivos
(1-especificidad) y sensibilidad:

```{r, fig.width = 5, fig.asp =0.9}
clasif_1 <- data.frame(
  corte = c('0.3','0.5','0.7','perfecto','azar'),
  tasa_falsos_pos=c(0.24,0.08,0.02,0,0.7),
  sensibilidad =c(0.66, 0.46,0.19,1,0.7))
ggplot(clasif_1, aes(x=tasa_falsos_pos, y=sensibilidad,
  label=corte)) + geom_point() + 
  geom_abline(intercept=0, slope=1) +
  xlim(c(0,1)) +ylim(c(0,1)) + geom_text(hjust=-0.3, col='red')+
  xlab('1-especificidad (tasa falsos pos)')

```



1. Nótese que agregamos otros dos clasificadores, uno perfecto, que tiene tasa de falsos positivos igual a 0 y sensibilidad igual a 1.
2. En esta gráfica, un clasificador G2 que está arriba a la izquierda de G1 domina a G1, pues tiene mejor especificidad y mejor sensi- bilidad. Entre los clasificadores 0.4, 0.5 y 0.7 de la gráfica, no hay ninguno que domine a otro.
3. Todos los clasificadores en la diagonal son equivalentes a un clasificador al azar. ¿Por qué? La razón es que si cada vez que vemos un nuevo caso lo clasificamos como positivo con probabilidad p fija y arbitraria. Esto implica que cuando veamos un caso positivo, la probabilidad de ’atinarle’ es de p (sensibilidad), y cuando vemos un negativo, la probabilidad de equivocarnos también es de p (tasa de falsos positivos). De modo que este clasificador al azar está en la diagonal.
4. ¿Qué podemos decir acerca de clasificadores que caen por debajo de la diagonal? Estos son clasificadores particularmente malos, pues existen clasificadores con mejor especificidad y/o sensibili- dad que son clasificadores al azar! Sin embargo, se puede construir un mejor clasificador volteando las predicciones, lo que cambia sensibilidad por tasa de falsos positivos.
5. ¿Cuál de los tres clasificadores es el mejor? En términos de la tasa de incorrectos, el de corte 0.5. Sin embargo, para otros propósitos puede ser razonable escoger alguno de los otros.

### Perfil de un clasificador binario y curvas ROC{-}


En lugar de examinar cada punto de corte por separado, podemos hacer el análisis de todos los posibles puntos de corte mediante la curva ROC (receiver operating characteristic, de ingeniería).
```{block2, type='comentario'}
 Para un problema de clasificación binaria, dadas estimaciones $\hat{p}(x)$, 
 la curva ROC grafica todos los pares de (1-especificidad, sensibilidad) para cada posible punto de corte $\hat{p}(x) > d$.
 ```

#### Ejemplo {-}

```{r, warnings=FALSE,message=FALSE}
library(tabplot)
mod_1 <- glm(type ~ glu, diabetes_ent, family = 'binomial')
diabetes_pr$probs_prueba_1 <- predict(mod_1, newdata = diabetes_pr,
                                      type = "response") 
head(arrange(diabetes_pr, desc(probs_prueba_1)))
tableplot(diabetes_pr, sortCol = probs_prueba_1)
```


La columna de probabilidad de la derecha nos dice en qué valores
podemos cortar para obtener distintos clasificadores. Nótese que
si cortamos más arriba, se nos escapan más positivos verdaderos
que clasificamos como negativos, pero clasificamos a más
negativos verdaderos como negativos. Lo opuesto ocurre 
cuando cortamos más abajo.

Vamos a graficar todos los pares (1-especificidad, sensibilidad)
para cada punto de corte $d$ de estas probabilidades.

```{r, message=FALSE, warning=FALSE}
library(ROCR)
pred_rocr <- prediction(diabetes_pr$probs_prueba_1, diabetes_pr$type) 
perf <- performance(pred_rocr, measure = "sens", x.measure = "fpr") 
graf_roc_1 <- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], 
                       d = perf@alpha.values[[1]])

ggplot(graf_roc_1, aes(x = tfp, y = sens, colour=d)) + geom_point() +
  xlab('1-especificidad') + ylab('Sensibilidad') 
```

En esta gráfica podemos ver todos los clasificadores posibles basados
en las probabilidades de clase. Podemos usar estas curvas como evaluación
de nuestros clasificadores, dejando para más tarde la selección del punto de
corte, si esto es necesario (por ejemplo, dependiendo de los costos de cada
tipo de error).

También es útil para comparar modelos. Consideremos el modelo de los datos
de diabetes que incluyen todas las variables:
```{r, warnings=FALSE,message=FALSE}
mod_2 <- glm(type ~ ., diabetes_ent, family = 'binomial')
diabetes_pr$probs_prueba_2 <- predict(mod_2, newdata = diabetes_pr,
                                      type = "response") 
head(arrange(diabetes_pr, desc(probs_prueba_2)))
tableplot(diabetes_pr, sortCol = probs_prueba_2)
```


Y graficamos juntas:

```{r}
library(ROCR)
pred_rocr <- prediction(diabetes_pr$probs_prueba_2, diabetes_pr$type) 
perf <- performance(pred_rocr, measure = "sens", x.measure = "fpr") 
graf_roc_2 <- data_frame(tfp = perf@x.values[[1]], sens = perf@y.values[[1]], 
                       d = perf@alpha.values[[1]])

graf_roc_2$modelo <- 'Todas las variables'
graf_roc_1$modelo <- 'Solo glucosa'
graf_roc <- bind_rows(graf_roc_1, graf_roc_2)

ggplot(graf_roc, aes(x = tfp, y = sens, colour = modelo)) + geom_point() +
  xlab('1-especificidad') + ylab('Sensibilidad') 
```

En este ejemplo, vemos que casi no importa que perfil de especificidad y sensibilidad busquemos: el clasificador que usa todas las variables
domina casi siempre al clasificador que sólo utiliza las variables de glucosa. 
La razón es que para cualquier punto de corte (con sensibilidad menor a 0.4) en el clasificador de una variable, existe otro clasificador en la curva roja (todas las variable), que domina al primero. La excepción es para clasificadores de valores de sensibilidad baja, con tasas de falsos positivos muy chicas: 
en este caso, el modelo de una variable puede ser ligeramente superior.

## Regresión logística para problemas de más de 2 clases

Consideramos ahora un problema con más de dos clases, de ma- nera que $G ∈ {1,2,...,K}$
($K$ clases), y tenemos $X = (X1 ...,Xp)$ entradas.
¿Cómo generalizar el modelo de regresión logística a este problema?
Una estrategia es la de uno contra todos:

En clasificación uno contra todos, hacemos

1. Para cada clase $g\in\{1,\ldots,K\}$ entrenamos un modelo de regresión
logística (binaria) $\hat{p}^{(g)}(x)$, tomando como positivos a los casos de 1
clase $g$, y como negativos a todo el resto. Esto lo hacemos como en las secciones anteriores, y de manera independiente para cada clase.

2. Para clasificar un nuevo caso $x$, 
calculamos 
$$\hat{p}^{(1)}, \hat{p}^{(2)},\ldots, \hat{p}^{(K)}$$

y clasificamos a la clase de máxima probabilidad
$$\hat{G}(x) = \arg\max_g \hat{p}^{(g)}(x)$$
Nótese que no hay ninguna garantía de que las probabilidades de clase
sumen 1, pues se trata de estimaciones independientes de cada clase. En este sentido, produce estimaciones que en realidad no satisfacen las propiedades del modelo de probabilidad establecido. Sin embargo, esta estrategia es simple y en 
muchos casos funciona bien.

### Regresión logística multinomial


Si queremos obtener estimaciones de las probabilidades de clase que sumen uno, entonces tenemos que contruir las estimaciones de cada clase de clase de manera conjunta.
Como vimos antes, tenemos que estimar, para cada $x$ y $g\in\{1,\ldots, K\}$,
las probabilidades condicionales de clase:
$$p_g(x) = P(G = g|X = x).$$

Consideremos primero cómo funciona el modelo de regresión logística (2 clases)

Tenemos que
$$p_1(x) = h(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p) =
\exp(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p)/Z
$$
y
$$p_2 (x) = 1/Z$$
donde $Z = 1 + \exp(\beta_0 + \beta_1x_1 + \ldots + \beta_p x_p)$.

Podemos generalizar para más de 2 clases usando una idea similar:

$$p_1(x) =  \exp(\beta_{0,1} + \beta_{1,1}x_1 + \ldots + \beta_{p,1} x_p)/Z$$

$$p_2(x) =  \exp(\beta_{0,2} + \beta_{1,2}x_2 + \ldots + \beta_{p.2} x_p)/Z$$
hasta
$$p_{K-1}(x) =  \exp(\beta_{0,{K-1}} + \beta_{1,{K-1}}x_2 + \ldots + \beta_{p,{K-1}} x_p)/Z$$
y 
$$p_K(x) = 1/Z$$

En este caso, para que las probabilidades sumen 1, necesitamos que
$$Z = 1 + \sum_{j=1}^{K-1}\exp(\beta_0^j + \beta_1^jx_2 + \ldots + \beta_p^j x_p)$$

Para ajustar coeficientes, usamos el mismo criterio de devianza de entrenamiento.
Buscamos minimizar:
$$D(\beta)=−2 \sum_{i=1}^N p_{g^{(i)}}(x^{(i)}),$$
Donde $\beta$ contiene todos los coeficientes organizados en un vector
de tamaño $(p+1)(K+1)$:
$$\beta = ( \beta_0^1, \beta_1^1, \ldots , \beta_p^1,  \beta_0^2, \beta_1^2, \ldots , \beta_p^2, \ldots \beta_0^{K-1}, \beta_1^{K-1}, \ldots , \beta_p^{K-1} )$$

Y ahora podemos usar algún método númerico para minimizar la devianza (por ejemplo,
descenso en gradiente).  Cuando
es muy importante tener  probabilidades bien calibradas, el enfoque multinomial
es más apropiado, pero muchas veces, especialmente si sólo nos interesa clasificar, los
dos métodos dan resultados similares.

### Interpretación de coeficientes

Los coeficientes mostrados en la parametrización de arriba se intrepretan
más fácilmente como comparaciones de la clase $j$ contra la clase $K$, pues

$$\log\left (\frac{p_g(x)}{p_K(x)}\right ) = \beta_{0,{g}} + \beta_{1,{g}}x_2 + \ldots + \beta_{p,{g}} x_p$$

Para comparar la clase $j$ con la clase $k$ notamos que

$$\log\left (\frac{p_j(x)}{p_k(x)}\right ) = 
(\beta_{0,{j}}- \beta_{0,{k}}) + (\beta_{1,{j}}-\beta_{1,{k}} )x_2 + \ldots + (\beta_{p,{j}} -\beta_{p,{k}})  x_p$$

Así que sólo hace falta restar los coeficientes. Nótese adicionalmente
que en la parametrización, podemos pensar que

$$\beta_{0,K} = \beta_{1,K} = \cdots = \beta_{p,K} = 0$$ 


### Ejemplo: Clasificación de dígitos con regresión multinomial

```{r}
library(readr)
digitos_entrena <- read_csv('datos/zip-train.csv')
digitos_prueba <- read_csv('datos/zip-test.csv')
names(digitos_entrena)[1] <- 'digito'
names(digitos_entrena)[2:257] <- paste0('pixel_', 1:256)
names(digitos_prueba)[1] <- 'digito'
names(digitos_prueba)[2:257] <- paste0('pixel_', 1:256)
```

En este ejemplo, usamos la función *multinom* de *nnet*, que usa
BFGS para hacer la optimización:
```{r}
library(nnet)
mod_mult <- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 20)
```

Checamos para diagnóstico la matriz de confusión **de entrenamiento**.

```{r}
table(predict(mod_mult), digitos_entrena$digito)
```


Ahora validamos con la muestra de prueba y calculamos error de clasificación:
```{r}
confusion_prueba <- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito)
confusion_prueba
sum(diag(confusion_prueba))/sum(confusion_prueba)
round(prop.table(confusion_prueba, 2),2)
```

El resultado no es muy bueno. Veremos más adelante mejores métodos para 
este problema. ¿Podemos interpretar el modelo?

Una idea es tomar los coeficientes y graficarlos según la estructura de
las imágenes:

```{r}
coefs <- coef(mod_mult)
coefs_reng <- coefs[1, , drop =FALSE]
coefs <- rbind(coefs_reng, coefs)
coefs[1 , ] <- 0
dim(coefs)
beta_df <- coefs[,-1] %>% as.data.frame %>% 
  mutate(digito = 0:(nrow(coefs)-1)) %>%
  gather(pixel, valor, contains('pixel')) %>%
  separate(pixel, into = c('str','pixel_no'), sep='_') %>%
  mutate(x = (as.integer(pixel_no)-1) %% 16, y = -((as.integer(pixel_no)-1) %/% 16))
head(beta_df)
```

Podemos cruzar la tabla con sí misma para hacer comparaciones:
```{r}
tab_coef <- beta_df %>% select(digito, x, y, valor)
tab_coef_1 <- tab_coef
names(tab_coef_1) <- c('digito_1','x','y','valor_1')
tab_cruzada <- full_join(tab_coef_1, tab_coef) %>% mutate(dif = valor_1 - valor)
tab_cruzada <- tab_cruzada %>% group_by(digito, digito_1) %>% 
  mutate(dif_s = (dif - mean(dif))/sd(dif)) %>%
  mutate(dif_p = pmin(pmax(dif_s, -2), 2))
```

```{r}
ggplot(tab_cruzada, aes(x=x, y=y)) + geom_tile(aes(fill = dif_p)) + 
  facet_grid(digito_1~digito)+scale_fill_distiller(palette = "Spectral")
```



### Discusión {-}

Nótese que no corrimos el modelo hasta convergencia. Vamos a hacerlo ahora:


```{r}
mod_mult <- multinom(digito ~ ., data = digitos_entrena, MaxNWt=100000, maxit = 500)
```


```{r, cache = TRUE}
confusion_prueba <- table(predict(mod_mult, newdata = digitos_prueba), digitos_prueba$digito)
confusion_prueba
sum(diag(confusion_prueba))/sum(confusion_prueba)
round(prop.table(confusion_prueba, 2),2)
```

Y nota que el error es más grande que cuando nos detuvimos antes. Discute en clase:

- Grafica los coeficientes para este segundo modelo
- ¿En cuál de los dos modelos es más fácil interpretar los coeficientes? ¿En cuál
es menor el error?
- ¿Cuál crees que es el problema de este segundo modelo comparado con el primero? ¿Por qué crees que sucede? ¿Cómo podríamos corregir este problema?

## Descenso en gradiente para regresión multinomial logística

Supondremos $K$ clases, 

```{r}
pred_ml <- function(x, beta){
  p <- ncol(x)
  K <- length(beta)/(p+1)
  beta_mat <- matrix(beta, K, p + 1 , byrow = TRUE)
  u_beta <- exp(as.matrix(cbind(1, x)) %*% t(beta_mat))
  Z <- apply(u_beta, 1, sum)
  p_beta <- u_beta/Z
  as.matrix(p_beta)
}
  
  
devianza_calc <- function(x, y){
  dev_fun <- function(beta){
    p_beta <- pred_ml(x, beta)
    p <- sapply(1:nrow(x), function(i) p_beta[i, y[i]+1])
   -2*sum(log(p))
  }
  dev_fun
}

grad_calc <- function(x_ent, y_ent){
  p <- ncol(x_ent)
  K <- length(unique(y_ent))
  #param_l <- (K-1)*(p+1)
  salida_grad <- function(beta){
    p_beta <-  pred_ml(x_ent, beta)
    ff <- factor(y_ent) 
    y_dummy <-  model.matrix(~-1+ff) 
    e_mat <-  y_dummy  - p_beta
    grad_out <- -2*(t(cbind(1,x_ent)) %*% e_mat)
    as.numeric(grad_out)
  }
  salida_grad
}
descenso <- function(n, z_0, eta, h_deriv){
  z <- matrix(0,n, length(z_0))
  z[1, ] <- z_0
  for(i in 1:(n-1)){
    z[i+1, ] <- z[i, ] - eta * h_deriv(z[i, ])
  }
  z
}
```


```{r}
x_ent <- digitos_entrena %>% select(contains('pixel')) %>% as.matrix
y_ent <- digitos_entrena$digito
beta <- runif(257*10)
dev_ent <- devianza_calc(x_ent, y_ent)
grad <- grad_calc(x_ent, y_ent)
dev_ent(beta)
```

```{r}
beta_2 <- beta 
epsilon <- 0.00001
beta_2[10] <- beta[10] + epsilon

(dev_ent(beta_2) - dev_ent(beta))/epsilon
```

```{r}
grad(beta)[10]
```